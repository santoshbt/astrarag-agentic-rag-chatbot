import logging

from crewai.tools import tool
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.groq import Groq
from llama_index.core import Settings
import chromadb

from src.agents_src.config.agent_settings import AgentSettings

# Get a logger for this module
logger = logging.getLogger(__name__)

# download & load embedding model
logger.info("Loading HuggingFace embedding model...")
embed_model = HuggingFaceEmbedding()


@tool
def rag_query_tool(query: str) -> dict:
    """
    Answers a query by retrieving relevant documents and generating a response.
    Returns both the generated answer and the source file names from which the information was retrieved.

    Args:
        query (str): The input query string to be processed.

    Returns:
        dict: A dictionary with the following keys:
            - 'answer': The generated answer string.
            - 'source_files': List of source file names used for retrieval.

    Notes:
        - Requires properly configured AgentSettings and access to the vector store.
        - The function loads the embedding model and LLM each time it is called.
    """

    settings = AgentSettings()
    vector_store_path = settings.VECTOR_STORE_DIR
    collection_name = settings.COLLECTION_NAME
    # Configure LLM
    Settings.llm = Groq(
        model=settings.MODEL_NAME,
        temperature=settings.MODEL_TEMPERATURE,
        api_key=settings.GROQ_API_KEY,
    )
    # Load Chroma collection
    db = chromadb.PersistentClient(path=vector_store_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    # connect to the vector store
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    # Load index from Chroma
    index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store,
        storage_context=storage_context,
        embed_model=embed_model
    )
    # Create the query engine
    query_engine = index.as_query_engine(similarity_top_k=3)
    # Pass the query to the query engine
    response = query_engine.query(query)

    # Detailed logging for debugging
    logger.info(f"\n=== RAG Query Tool Debug ===")
    logger.info(f"Input Query: {query}")
    logger.info(f"Number of source nodes retrieved: {len(response.source_nodes) if hasattr(response, 'source_nodes') else 0}")

    if hasattr(response, 'source_nodes') and response.source_nodes:
        for i, node in enumerate(response.source_nodes):
            node_text = node.text[:150] if hasattr(node, 'text') else str(node)[:150]
            logger.info(f"  Retrieved Node {i+1}: {node_text}...")

    logger.info(f"Generated Answer Length: {len(response.response) if response.response else 0}")
    if response.response:
        logger.info(f"Generated Answer (first 200 chars): {response.response[:200]}...")
    else:
        logger.warning(f"Generated Answer is EMPTY!")

    source_file_names = {m.get("file_name") for m in getattr(response, "metadata", {}).values()}
    logger.info(f"Source files extracted: {source_file_names}")
    logger.info(f"=== End Debug ===\n")

    return {"answer": response.response,
            "source_files": list(source_file_names)}


def rag_query_tool_sync(query: str) -> dict:
    """
    Synchronous wrapper for rag_query_tool that can be called directly from Python.
    This is the version used by the RAGAS evaluator (avoids CrewAI Tool wrapper).

    Args:
        query (str): The input query string to be processed.

    Returns:
        dict: A dictionary with 'answer' and 'source_files' keys.
    """
    settings = AgentSettings()
    vector_store_path = settings.VECTOR_STORE_DIR
    collection_name = settings.COLLECTION_NAME
    # Configure LLM
    Settings.llm = Groq(
        model=settings.MODEL_NAME,
        temperature=settings.MODEL_TEMPERATURE,
        api_key=settings.GROQ_API_KEY,
    )
    # Load Chroma collection
    db = chromadb.PersistentClient(path=vector_store_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    # connect to the vector store
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    # Load index from Chroma
    index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store,
        storage_context=storage_context,
        embed_model=embed_model
    )
    # Create the query engine
    query_engine = index.as_query_engine(similarity_top_k=3)
    # Pass the query to the query engine
    response = query_engine.query(query)

    # Detailed logging for debugging
    logger.info(f"\n=== RAG Query Tool Debug ===")
    logger.info(f"Input Query: {query}")
    logger.info(f"Number of source nodes retrieved: {len(response.source_nodes) if hasattr(response, 'source_nodes') else 0}")

    if hasattr(response, 'source_nodes') and response.source_nodes:
        for i, node in enumerate(response.source_nodes):
            node_text = node.text[:150] if hasattr(node, 'text') else str(node)[:150]
            logger.info(f"  Retrieved Node {i+1}: {node_text}...")

    logger.info(f"Generated Answer Length: {len(response.response) if response.response else 0}")
    if response.response:
        logger.info(f"Generated Answer (first 200 chars): {response.response[:200]}...")
    else:
        logger.warning(f"Generated Answer is EMPTY!")

    source_file_names = {m.get("file_name") for m in getattr(response, "metadata", {}).values()}
    logger.info(f"Source files extracted: {source_file_names}")
    logger.info(f"=== End Debug ===\n")

    return {"answer": response.response,
            "source_files": list(source_file_names)}


# For direct testing, uncomment the code below and comment out @tool.
# When using CrewAI, uncomment @tool and comment out the test code.

# output = rag_query_tool(query="Explain Ecosystem and Evolution.")
# print(output)
# print(output["answer"])
# print(output["source_files"])
